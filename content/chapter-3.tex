\chapter{Data Quality from Disagreement}
\label{chap:maj-vote}
\addcontentsline{lof}{chapter}{\textsc{Chap.~\ref{chap:maj-vote}: Data Quality from Disagreement}}
\addcontentsline{lot}{chapter}{\textsc{Chap.~\ref{chap:maj-vote}: Data Quality from Disagreement}}

\begin{chapquote}{Mark Twain, \textsc{Notebook, 1904}}
Whenever you find yourself on the side of the majority, it is time to reform (or pause and reflect).
\end{chapquote}

% \begin{chapquote}{Bertrand Russell, \textsc{The Philosophy of Logical Atomism}}
% Everything is vague to a degree you do not realize till you have tried to make it precise, and everything precise is so remote from everything that we normally think, that you cannot for a moment suppose that is what we really mean when we say what we think.
% \end{chapquote}

The process of gathering ground truth data through human annotation is a major bottleneck in the use of information extraction methods for populating the Semantic Web. Crowdsourcing-based approaches are gaining popularity in the attempt to solve the issues related to volume of data and lack of annotators. Typically these practices use inter-annotator agreement as a measure of quality. However, many domains contain ambiguity in the data, as well as a multitude of perspectives of the information examples. In this chapter, we present an empirically derived methodology for efficiently gathering of ground truth data in a diverse set of use cases covering a variety of domains and annotation tasks. Central to our approach is the use of CrowdTruth metrics that capture inter-annotator disagreement. We show that measuring disagreement is essential for acquiring a high quality ground truth. We achieve this by comparing the quality of the data aggregated with CrowdTruth metrics with majority vote, over a set of diverse crowdsourcing tasks: Medical Relation Extraction, Twitter Event Identification, News Event Extraction and Sound Interpretation. We also show that an increased number of crowd workers leads to growth and stabilization in the quality of annotations, going against the usual practice of employing a small number of annotators.


%While in Chapter~\ref{chap:med-rel-ex} we have shown that the preservation of inter-annotator disagreement still results in high quality ground truth data, in this chapter we investigate more concretely how does disagreement influences data quality across a variety of tasks, as compared with voting aggregation methods that only take into account the opinion of the majority.

This chapter will appear in publication as \textit{Empirical Methodology for Crowdsourcing Ground Truth} in the Semantic Web Journal and was co-authored by Oana Inel, Benjamin Timmermans, Carlos Ortiz, Robert-Jan Sips, Lora Aroyo and Chris Welty.~\cite{dumitracheempirical}

\section{Introduction}
\label{sec:introduction}

% \textbf{RQ2:} \textit{How does allowing disagreement in diverse crowdsourcing tasks influence the quality of the data?}
Knowledge base curation, or the task of populating knowledge bases, is one of the main research challenges of crowdsourcing the Semantic Web~\cite{sarasua2015crowdsourcing}. Knowledge base curation can be done either manually, by asking annotators to populate the knowledge graph by manually extracting triples from unstructured data, or automatically by using information extraction methods that are trained and evaluated on ground truth collected from human annotators. In both cases, the process of gathering the human annotations is the a bottleneck in the entire knowledge base population process. The traditional approach to gathering human annotation is to employ experts to perform annotation tasks~\cite{welty2012query}, which is a costly and time consuming process. In addition, expert annotators are not always available for specific tasks such as open domain question-answering or news events, while many annotation tasks can require multiple interpretations that a single annotator cannot provide~\cite{aroyo2012harnessing}.

As a solution to those problems, crowdsourcing has become a mainstream approach. It has proved to provide good results in multiple domains: annotating cultural heritage prints~\cite{oosterman2014crowdsourcing}, medical relation annotation~\cite{aroyo2013measuring}, ontology evaluation~\cite{noy2013mechanical}. Following the central feature of volunteer-based crowdsourcing introduced by~\cite{von2009human} that majority voting and high inter-annotator agreement~\cite{Carletta1996} can ensure truthfulness of resulting annotations, most of those approaches are assessing the quality of their crowdsourced data based on the hypothesis~\cite{nowak2010reliable} that there is only one right answer to each question.

However, in Chapter~\ref{chap:med-rel-ex} we have shown that the preserving inter-annotator disagreement as part of the ground truth can still result in high quality data, at least for the case of medical relation extraction. Similar results were observed in collecting annotations for text~\cite{poesio2005reliability,chang2016linguistic}, sounds~\cite{doi:10.1080/09298215.2016.1200631} and images~\cite{schaekermann2016,cheplygina2018crowd}, where it was found that disagreement between annotators is not just a result of poor quality work, and can actually be an indicator for other properties of the data, such as ambiguity and uncertainty~\cite{aroyo2018aimag}. Previous experiments we performed~\cite{aroyo2013crowd} also identified issues with the assumption of the one truth: inter-annotator disagreement is usually never captured, either because the number of annotators is too small to capture the full diversity of opinion, or because the crowd data is aggregated with metrics that enforce consensus, such as majority vote.  These practices create artificial data that is neither general nor reflects the ambiguity inherent in the data.

In this chapter, we build on these findings and investigate more precisely \textit{how does allowing disagreement in crowdsourcing tasks influence the quality of the data} (\textbf{RQ2}). To answer this question, we investigate across a variety of tasks and domains (\textit{Medical Relation Extraction}, \textit{Twitter Event Identification}, \textit{News Event Extraction} and \textit{Sound Interpretation}). Also we perform an evaluation in comparison with voting aggregation methods that only take into account the opinion of the majority.

To capture and interpret inter-annotator disagreement, we employ the \textit{CrowdTruth} methodology for crowdsourcing human annotation~\cite{aroyo2014threesides}. Through the use of CrowdTruth aggregation metrics, the interpretations collected from the crowd are transformed into explicit semantics for the various tasks presented in this chapter -- i.e. relations expressed in sentences, topics / events expressed in tweets and news articles, words describing sounds -- thus enabling knowledge base curation for these specific tasks.  We prove that capturing disagreement is essential for acquiring high quality semantics.  We achieve this by comparing the quality of the data aggregated with CrowdTruth metrics with majority vote, a method which enforces consensus among annotators.  By applying our analysis over a set of diverse tasks we show that, even though ambiguity manifests differently depending on the task (e.g. each task has an optimal number of workers necessary to capture the full spectrum of opinions), our theory of inter-annotator disagreement as a property of ambiguity is generalizable for any semantic annotation crowdsourcing task.

The chapter makes the following contributions:

\begin{enumerate}

\item \textit{comparative analysis of crowdsourcing aggregation methods:} we compare the performance of \textit{ambiguity-aware CrowdTruth metrics} and \textit{consensus - enforcing metrics} over a diverse set of crowdsourcing tasks (Sections \ref{sec:results} \& \ref{sec:discussion});

\item \textit{stability of crowd results:} we show in several crowdsourcing tasks that \textit{an increased number of crowd workers leads to growth and stabilization in the quality of annotations}, going against the usual practice of employing a small number of annotators (Sections \ref{sec:results} \& \ref{sec:discussion});

\item \textit{measuring quality in open-ended tasks:} we present an extension to the CrowdTruth methodology that allows the ambiguity-aware CrowdTruth metrics to deal \textit{both with open-ended and closed tasks} (Sections \ref{sec:methodology} \& \ref{sec:experimental_setup}), as opposed to the initial version of the CrowdTruth metrics which only processed closed tasks;

\item \textit{semantics of ambiguity:} applying the CrowdTruth methodology we collect richer data that allows to reason about ambiguity of content (in all modality formats, e.g. images, videos and sounds), which is intrinsically relevant to the Semantic Web community (Section~\ref{sec:discussion}).

\end{enumerate}


\section{CrowdTruth Methodology}
\label{sec:methodology}

In this section, we describe the CrowdTruth \textit{methodology} version 1.1, for aggregating crowdsourcing data, which offers methods to aggregate both closed an open-ended tasks. Version 1.1 presented in this chapter is a generalization of the initial version 1.0 of CrowdTruth~\cite{inel2014crowdtruth}.

In Section~\ref{sec:results} we use a number of annotation tasks in different domains  to illustrate its use and gather experimental data to prove the main claim of this research - CrowdTruth methodology provides a viable alternative to traditional consensus-based majority vote crowdsourcing and expert-based ground truth collection. The elements of the CrowdTruth methodology are:
\begin{itemize}
\item annotation modeling with the \emph{triangle of disagreement};
\item quality \emph{metrics} for media units (input data), annotations and crowd workers;
\item identification of workers with low quality annotations.
\end{itemize}

Each of these elements is applicable across a variety of domains, content modalities, \emph{e.g.}, text, sounds, images and videos and annotation tasks, \emph{e.g.}, closed and open-ended annotations. The following sub-sections briefly introduce the overview of the methodology elements.

\subsection{CrowdTruth quality metrics}
\label{subsec:metrics}

Measuring quality in CrowdTruth is done with  the triangle of disagreement model (based on the triangle reference \cite{knowlton1966definition}), which links together media units, workers, and annotations, as seen in Fig.\ref{fig:triangle_of_reference}. It allows us to assess the quality of each worker, the clarity of each media unit, and the ambiguity, similarity and frequency of each annotation. This model makes it possible to express how the ambiguity in any of the corners disseminates and influences the other components of the triangle. For example, an unclear sentence or an ambiguous annotation scheme would cause more disagreement between workers \cite{aroyo2014threesides}, and thus, both need to be accounted for when measuring the quality of the workers. 

 \begin{figure}[!hpt]
 	\centering
 		\includegraphics[width=0.5\linewidth]{img/triangle.png}
 	\caption{Triangle of disagreement.}
 	\label{fig:triangle_of_reference}
 \end{figure}

The CrowdTruth quality metrics~\cite{aroyo2014threesides} are designed to capture inter-annotator disagreement in crowdsourcing. The CrowdTruth metrics were developed for \textit{closed tasks}, i.e. multiple choice tasks, where the annotation set is known before running the crowdsourcing task. In Chapter~\ref{sec:chap2_metrics}, we employed them to aggregate the results from the crowd for the task of medical relation extraction from sentences, where the sentences represent the media units and the relations represent the annotations in the triangle of disagreement model.

In this chapter, we present a generalized and extended version of these metrics (version 1.1), that can be used for both \textit{closed tasks} as well as \textit{open-ended tasks} (i.e. the annotation set is not known beforehand, and the workers can freely select all the choices that apply). The code for the CrowdTruth version 1.1 metrics is available at: \url{https://git.io/fA3Mq}.

The quality of the crowdsourced data is measured using a \textit{vector space representation} of the crowd annotations. For \emph{closed tasks}, the annotation vector contains the given answer options in the task template, which the crowd can choose from. For example, the template of a \emph{closed task} can be composed of a multiple choice question, which appears as a list checkboxes or radio buttons, thus, having a finite list of options to choose from.

While for closed tasks the number of elements in the annotation vector is known in advance, for \emph{open-ended tasks} the number of elements in the annotation vector can only be determined when all the judgments for a media unit have been gathered. An example of such a task can be highlighting words or word phrases in a sentence, or as an input text field where the workers can introduce keywords. In this case the answer space is composed of all the unique keywords from all the workers that solved that media unit. As a consequence, all the media units in a closed task have the same answer space, while for open-ended tasks the answer space is different across all the media units. The construction of an open-ended annotation vector is shown in Table~\ref{tab:example_annotation}.

\begin{table}[htb!]
    \scalebox{0.85}{
	\begin{tabular}{rccccc}
    \toprule
    \textsc{worker annotations} & \textit{dog barking} & \textit{walking} & \textit{animal} & \textit{echo} & \textit{loud} \\ \hline
    \textsc{media unit -- annotation score} &  0.47 & 0.31 & 0.79 & 0.15 & 0.15 \\ %\hline
    \textsc{media unit vector} & 3 & 2 & 5 & 1 & 1 \\ %\hline
    \textsc{majority vote} &  0 & 0 & 1 & 0 & 0 \\ \bottomrule
    \end{tabular}
    }
	\caption {Consider an open-ended sound annotation task where 10 workers have to describe a given sound with keywords. The media unit for this task is a sound, the annotation set contains all the keywords workers provide for a sound. The table shows the media unit metrics, as well as the majority vote score for the media unit.}
    \label{tab:example_annotation}
\end{table}

Although the answer space for open-ended tasks is not known from the beginning, it is still possible to deduce a finite answer space. To achieve this, we added an \textit{answer space dimensionality reduction step} to the methodology for open-ended tasks. Additional goals of this step are to reduce redundancy in the answer space through similarity clustering (e.g. by making sure that synonymous words do not count as disagreement between annotators), and to keep the vector space representation small enough so that the CrowdTruth quality metrics still produce meaningful values. The method for performing dimensionality reduction is dependent on the annotation task itself. % Different methods are needed when dealing with, for instance, tasks that give free text input to the crowd vs. tasks that ask the crowd to combine various)


In the annotation vector, each answer option is a boolean value, showing whether the worker annotated that answer or not. This allows the annotations of each worker on a given media unit to be aggregated, resulting in a \textit{media unit vector} that represents for each option how often it was annotated.

Three core \textit{worker metrics} are defined to differentiate between low-quality and high-quality workers. \emph{Worker-Worker Agreement} ($wwa$) measures the pairwise agreement between two workers across all media units they annotated in common - indicating how close a worker performs compared to workers solving the same task. \emph{Worker-Media Unit Agreement} ($wma$) measures the similarity between the annotations of a worker and the aggregated annotations of the rest of the workers. The average of this metric across all the media units solved gives a measure of how much a worker disagrees with the crowd in the context of all media units. \emph{Average annotations per media unit} ($na$) measures for each worker the total number of annotations they chose per media unit, averaged across all media units they annotated. Since in many tasks workers can choose all the possible annotations, a low quality worker can appear to agree more with the rest of the workers by repeatedly choosing multiple annotations, thus increasing the chance of overlap.

Two \textit{media unit metrics} are defined to assess the quality of each unit. In this chapter, we focus on the \emph{Media Unit-Annotation Score} -- the core CrowdTruth metric, used to measure the clarity with which the media unit expresses a given annotation. This metric is computed for each media unit and each possible annotation as the cosine between the media unit vector and the unit vector for each possible annotation.  This metric is used in evaluating the quality of the CrowdTruth annotations.


\subsection{Spam Removal}
\label{subsec:spam_removal}

After collecting the crowd annotations, but before the evaluation of the data, we perform spam removal.  The purpose of this step is to identify the adversarial and low quality workers -- e.g. those workers that always pick the same annotations, regardless of the unit. Once identified, the spam workers are removed from the dataset, and their annotations are not used in the evaluation.  The methodology for spam removal is based on our previous work in~\cite{soberon2013}, extended in this chapter to work also for open-ended tasks. 

We identify the low quality workers by applying the core CrowdTruth worker metrics, the worker-worker agreement ($wwa$), worker-media unit agreement ($wma$) and the average number of annotations ($na$) submitted by a worker for one sentence. The first two metrics are used to model the extent to which a given worker agrees with the other annotators. The purpose is not to penalize disagreement with the majority, but rather to identify outliers, \emph{i.e.}, workers that are in constant disagreement. For \emph{closed tasks} where the semantics of the annotations in the answer space could rarely overlap, it is unlikely that a large number of possible annotations will occur for the same media unit. Therefore, the number of annotations per sentence can also indicate spam behavior. 

In \emph{open-ended tasks} we apply the same approach. However, we need to acknowledge the fact that open-ended tasks are more prone to disagreement due to the large answer space and thus, the overall agreement between the workers can occur with lower values. Thus, we do not have predefined values for identifying the low-quality workers, but for every task or job we use the following main heuristic: given worker $w$, if the agreement $wwa(w)$, $wsa(w)$ and optionally, annotations per sentence $na(w)$, parameters do not fall within the standard deviation for the task, then worker $w$ is marked as a spammer. To confirm the validity of this metrics we also perform manual evaluation based on sampling of the results.

Based on the specificity of each task, closed or open-ended, the effort required to pick different annotations might vary. For instance, when no good annotation exists in the media unit, the time to complete the annotation is considerably reduced. This can bias the workers towards selecting the option that requires the least work. In order to prevent this, we introduce \textit{in-task effort consistency checks}. Such annotations do not count towards building the ground truth, and are used to reduce the bias from picking the quickest option. For instance, when stating that no annotation is possible in the media unit, the workers also have to write an explanation in a text box for why no annotation were provided.

\begin{table}[!htp]
	\scalebox{0.8}{
	\begin{tabular}{rccl}
    \toprule
    \textsc{Task} & \textsc{Type} & \textsc{Media Unit} & \textsc{Annotations} \\
    \toprule
    & \multirow{5}{*}{closed} & \multirow{5}{*}{sentence} & medical relations: \textit{cause}, \textit{treat}, \\
    Medical &  &  & \textit{prevent}, \textit{symptom}, \textit{diagnose},  \\
    Relation & & & \textit{side effect}, \textit{location} \textit{manifestation}, \\
    Extraction & & & \textit{contraindicate}, \textit{is a}, \textit{part of}, \\
    & & & \textit{associated with}, \textit{other}, \textit{none} \\ \hline
    
    & \multirow{8}{*}{closed} & \multirow{8}{*}{tweet} & tweet events: \textit{FIFA World Cup 2014}, \\
    & & & \textit{Davos world economic forum 2014},  \\
     & & & \textit{Islands disputed between China and Japan}, \\
    Twitter & & & \textit{2014 anti-China protests in Vietnam}, \\
    Event & & & \textit{Korean MV Sewol ferry ship sinking},  \\
    Identification & & & \textit{Japan whaling and dolphin hunting},  \\
     & & & \textit{Disappearance of flight MH370}, \\ 
    & & & \textit{Ukraine crisis 2014}, \textit{none of the above} \\ \hline
    
    News & \multirow{3}{*}{open-ended} & \multirow{3}{*}{sentence} & \multirow{3}{*}{words in the sentence} \\
    Event & & & \\ 
    Extraction & & & \\ \hline
    Sound & \multirow{2}{*}{open-ended} & \multirow{2}{*}{sound} & \multirow{2}{*}{tags describing sound} \\   
    Interpretation & & & \\ %\hline
    \bottomrule
    \end{tabular}
    }
    
	\caption {Crowdsourcing task details.}
    \label{tab:crowd_data}
\end{table}


\section{Experimental Setup}
\label{sec:experimental_setup}

The aim of the crowdsourcing experiments described and analyzed in this chapter is to show that the CrowdTruth ambiguity-aware crowdsourcing approach produces data with a higher quality than the traditional majority vote where consensus among annotators is enforced. In order to show this, we perform an experiment over a set of four diverse crowdsourcing tasks: 
\begin{itemize}
    \item two closed tasks, i.e. \textit{Medical Relation Extraction}, \textit{Twitter Event Identification},
    \item two open-ended tasks, i.e. \textit{News Event Extraction} and \textit{Sound Interpretation}.
\end{itemize}
These tasks were picked from diverse domains (medical, sound, open), to aid in the generalization of our results.  To evaluate the quality of the crowdsourcing data, we constructed a trusted judgments set by combining expert and crowd annotations. The rest of this section describes the details of the crowdsourcing tasks, trusted judgments acquisition process, as well as the evaluation methodology we employed.

\subsection{Crowdsourcing Overview}
\label{subsec:detasets}

% \begin{figure}[!tb]
% \centering
% \label{fig:taskdesigns}
% \begin{subfigure}{.5\textwidth}
% \includegraphics[width=\linewidth]{img/relex}
% \caption{Medical Relation Extraction}
% \label{fig:screenshot_medical}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
% \includegraphics[width=\linewidth]{img/tweets.png}
% \caption{Twitter Event Identification}
% \label{fig:screenshot_tweets}
% \end{subfigure}
% \begin{subfigure}{.5\textwidth}
% \includegraphics[width=\linewidth]{img/sound-task.png}
% \caption{Sound Interpretation}
% \label{fig:screenshot_sounds}
% \end{subfigure}%
% \begin{subfigure}{.5\textwidth}
% \includegraphics[width=\linewidth]{img/news_event_extraction.png}
% \caption{News Event Extraction}
% \label{fig:screenshot_news}
% \end{subfigure}
% \caption{Templates of the crowdsourcing tasks.}
% \end{figure}


Tables \ref{tab:crowd_data} and \ref{tab:crowd_tasks} present an overview of the crowdsourcing tasks, as well as the datasets used. The results of the crowdsourcing tasks were processed with the use of CrowdTruth metrics (Sec.~\ref{subsec:metrics}), and we removed consistently low quality workers based on the spam removal procedure (Sec~\ref{subsec:spam_removal}). The tasks were implemented and ran on Figure Eight\footnote{\url{https://figure-eight.com/}} (formerly known as CrowdFlower). The templates are available on the CrowdTruth platform\footnote{tasks marked with $*$: \url{https://github.com/CrowdTruth/CrowdTruth/wiki/Templates}}.

\begin{table}[!htp]
	\scalebox{0.8}{
	\begin{tabular}{r c c c c c}
    \toprule
    \textsc{Task} & \textsc{Source} & \textsc{Has} & \textsc{Media} & \textsc{Workers/} & \textsc{Cost/} \\
    &  & \textsc{Expert} & \textsc{Units} & \textsc{Unit} & \textsc{Judgment} \\
    \toprule
    Medical Relation Extraction & PubMed & yes & 975 & 15 & \$0.05  \\
    Twitter Event Identification & Twitter & no & \np{3019} & 7 & \$0.02 \\
    News Event Extraction & TimeBank & yes & \np{200} & 15 & \$0.02 \\
    Sound Interpretation & Freesound & yes  & \np{284} & 10 & \$0.01 \\ %\hline
    \bottomrule
    \end{tabular}
    }
	\caption{Crowdsourcing task data.}
    \label{tab:crowd_tasks}
\end{table}

The payment per judgment was determined through a series of pilot runs of the tasks where we started with a \$0.01 cost per judgment, and then gradually increased the payment until a majority of Figure Eight workers rated our tasks as having fair payments. As a result, we were able to get a constant stream of workers to participate in the tasks. The values shown in Table~\ref{tab:crowd_tasks} show the final cost per judgment we reached after the pilot runs. Since crowd pay has a complex effect on the quality of the annotation~\cite{mao2013volunteering}, and in order to remove confounding factors, judgments collected with costs lower than those in Table~\ref{tab:crowd_tasks} were left out of this evaluation. In total, it took two months to perform the pilot runs and then collect the judgments for all of the tasks.

The number of workers per media unit was determined experimentally with the goal of capturing all possible results from the crowd and stabilizing the quality of the annotations; this process is explained at length further on in Section~\ref{sec:results}, with the results of the experiment shown in Figure~\ref{fig:f1_workers}.

The \textbf{Medical Relation Extraction dataset} consists of 975 sentences extracted from PubMed\footnote{\url{http://www.ncbi.nlm.nih.gov/pubmed}} article abstracts. The sentences were collected using distant supervision~\cite{mintz2009distant}, a method that picks positive sentences from a corpus based on whether known arguments of the seed relation appear together in the sentence (\emph{e.g.}, the $treat$ relation occurs between the terms $antibiotics$ and $typhus$, so find all sentences containing both and repeat this for all pairs of arguments that hold). The MetaMap parser~\cite{aronson2001effective} was used to extract medical terms from the corpus and the UMLS vocabulary~\cite{bodenreider2004unified} was used for mapping terms to categories, and relations to term types. The intuition of distant supervision is that since we know the terms are related, and they are in the same sentence, it is more likely that the sentence expresses a relation between them (than just any random sentence). We started with a set of 8 UMLS relations important for clinical decision making~\cite{P14-1078}, that became the seed in distant supervision, but this chapter only discusses results for the relations $cause$ and $treat$, as these were the only relations for which we could also collect expert annotations. The expert judgment collection is detailed in Section~\ref{subsec:expert_data}.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.8\linewidth]{img/relex}
\caption{Medical relation extraction task template (\url{https://git.io/fhxfN}).}
\label{fig:screenshot_medical}
\end{figure}

The \emph{medical relation extraction task} (see Figure \ref{fig:screenshot_medical}) is a \textit{closed task}. The crowd is given a medical sentence with the two highlighted terms collected with distant supervision, and is then asked to select from a list all relations that are expressed between the two terms in the sentence. The relation list contains eight UMLS\footnote{\url{https://www.nlm.nih.gov/research/umls/}} relations, as well as \textit{is a}, \textit{part of}, \textit{associated with}, \textit{other}, \textit{none} relations, added to make the choice list complete. Multiple choices are allowed in this task. To reduce the bias of selecting $none$, we also added an in-task effort consistency check by asking workers to explain in a text box why no relation is possible between the terms. The task results are processed into an annotation vector containing a component for each of the relations. A detailed description of the crowdsourcing data collection is given in \cite{DBLP:journals/corr/DumitracheAW17}.

The \textbf{Twitter Event Identification dataset} consists of 3,019 English tweets from 2014, crawled from Twitter. The tweets are selected as been relevant to eight events, such as, ``Japan whale hunt'', ``China Vietnam relation'' among other controversial events. The dataset was created by querying a Twitter dataset from 2014 with relevant phrases for each of the eight events, \emph{e.g.}, ``Whaling Hunting'', ``Anti-Chinese in Vietnam''. The \emph{Twitter event identification task} (see Figure \ref{fig:screenshot_tweets}) is a \textit{closed task}. The crowd is asked to choose for each tweet the relevant events out of the list of eight, as well as to highlight for each of the relevant events the event mentions in the tweet. The crowd could also pick that none of the events was present in the tweet. Multiple choices of events were permitted. Since tweets and tweet annotations typically are not done by experts, we did not collect expert data for this task. To reduce the bias of selecting no event, we also added an in-task effort consistency check by asking workers to explain in a text box why none of the events is present in the tweet. The task results are processed into an annotation vector containing a component for each of the events.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.8\linewidth]{img/tweets.png}
\caption{Twitter event identification task template (\url{https://git.io/fhxf5}).}
\label{fig:screenshot_tweets}
\end{figure}

The \textbf{News Event Extraction dataset} consists of 200 randomly selected English sentences from the English TimeBank corpora~\cite{pustejovsky2003timebank}, which were also presented in \cite{CASELLI16.966}. The \emph{news event extraction} (see Figure \ref{fig:screenshot_news}) is an \textit{open-ended task}. The crowd receives an English sentence, and is asked to highlight words or word phrases (multiple words) that describe an event or a time expression. For each sentence, the crowd is allowed to highlight a maximum of 30 event expressions or time expressions. For the purpose of this research we only focus on evaluating the extraction of event expressions. We define an \emph{event} as something that happened, is happening, will or happen. On this dataset we employed expert annotators as described in Section \ref{subsec:expert_data}. To reduce the bias of selecting fewer events than actually expressed in the task, we implemented an in-task effort consistency check by asking workers that annotated 3 events or less to explain in a text box why no other events are expressed in the sentence. As part of the \textit{answer set dimensionality reduction step}, we removed the stop words from the sentence (we consider that the stop words are not meaningful for our analysis and they could add unsubstantial disagreement), and split the expressions collected from the crowd into words.  The annotation vector is composed of the words in the sentence, where a word is selected in the worker vector if it appears in at least one of the expressions identified by the worker.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.8\linewidth]{img/news_event_extraction.png}
\caption{News event extraction task template (\url{https://git.io/fhxfF}).}
\label{fig:screenshot_news}
\end{figure}

The \textbf{Sound Interpretation dataset} consists of 284 unique sounds sampled from the Freesound\footnote{\url{https://www.freesound.org/}} online database. All these recordings and their metadata are freely accessible through the Freesound API\footnote{\url{https://www.freesound.org/docs/api/}}. We focused on SoundFX sounds, \emph{i.e.}, sound effects category, as classified by \cite{font2014audio}. The \emph{Sound interpretation task} (see Figure \ref{fig:screenshot_sounds}) is an \textit{open-ended task}, where the crowd is asked to listen to three sounds and provide for each sound a comma separated list of keywords that best describe what they heard. For each sound, any number of answers is possible. In the \textit{answer set dimensionality reduction step}, the annotated keywords were clustered syntacticly using spell checking and stemming, and semantically using a word2vec model~\cite{mikolov2013distributed} pre-trained on the Google News corpus.  The annotation vector contains a component for each of the keywords used to describe the sound, after clustering. A detailed description of the crowdsourcing data collection and processing is given in \cite{miltenburg2016}. For this dataset we also collected expert annotations from the sound creators as described in Section \ref{subsec:expert_data}.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.8\linewidth]{img/sound-task.png}
\caption{Sound interpretation task template (\url{https://git.io/fhxfb}).}
\label{fig:screenshot_sounds}
\end{figure}


\subsection{Evaluation Methodology}
\label{subsec:evaluation}

The purpose of the evaluation is to determine the quality of the annotations generated with CrowdTruth ambiguity-aware aggregating metrics. To this end, we label each media unit and annotation pair with its media unit-annotation score (see Section \ref{subsec:metrics}), and compare it with three other methods for labeling the data, as described below:

\begin{itemize}
\item \textbf{Majority vote}:  Each media unit-annotation pair receives either a positive or a negative label, according to the decision of the majority of crowd workers. For each annotation performed by a crowd worker over a given media unit, we calculate the ratio of workers that have selected this annotation over the total number of workers that have annotated the unit, and assess whether it is greater or equal to 0.5. This allows for multiple annotations to be picked for one media unit. For some units, however, none of the annotations were picked by half or more of the workers. This is especially the case for open-ended tasks, such as sound interpretation, where workers put in a large number of annotations, and agreement is seldom. In these situations, we picked the annotations that were selected by the most workers (even if they do not constitute more than half). An example of the majority vote aggregation is shown in Table~\ref{tab:example_annotation}.

\item \textbf{Single}:  Each media unit-annotation pair receives either a positive or a negative label, according to the decision of a single crowd worker. For every media unit, this score was randomly sampled
from the set of workers annotating it. Judgments from workers labeled as spammers were not employed. While a single annotator is not used as often as the majority vote in traditional crowdsourcing, we use this dataset as a baseline for the crowd, to show that having more annotators generates better quality data.

\item \textbf{Expert}: Each media unit-annotation pair receives either a positive or a negative label, according to the expert decision. The details of how expert data was collected for each tasks are discussed in Section \ref{subsec:expert_data}.
\end{itemize}

The \emph{evaluation of the quality of the CrowdTruth method} was done by computing the micro-F1 score over each task. The micro-F1 score was used in order to treat each case equally, without giving advantage to annotations that appear less frequently in our datasets. Using the trusted judgments collected according to Section~\ref{subsec:expert_data}, we evaluate each media unit -- annotation pair as either a true positive, false positive etc. We compute the value of the micro-F1 score using the following formulas for the micro precision (Equation \ref{eq:precision}) and micro recall (Equation \ref{eq:recall}):

\begin{equation}
P_{micro} = \frac{\sum_{i=1}^{n}{TP_i}}{\sum_{i=1}^{n}{TP_i} + \sum_{i=1}^{n}{FP_i}}
\label{eq:precision}
\end{equation}

\begin{equation}
R_{micro} = \frac{\sum_{i=1}^{n}{TP_i}}{\sum_{i=1}^{n}{TP_i} + \sum_{i=1}^{n}{FN_i}}
\label{eq:recall}
\end{equation}

where $TP_i$, $FP_i$, $FN_i$, with $i$ from 1 to $n$ (the number of media units in the dataset), represent the number of true positive, false positive and false negative annotations for media unit $i$. Finally, the micro-F1 score is computed as the harmonic mean of the micro-precision and micro-recall.

An important variable in the evaluation is the \textit{media unit-annotation score threshold} for differentiating between a negative and a positive classification. Traditional crowdsourcing aims at reducing disagreement, and therefore corresponds to high values for this threshold. Lower values means accepting more disagreement in the classification of positive answers by the crowd. In our experiments, we tried a range of threshold values for each task, to investigate with which one we achieve the best results. The media unit-annotation score threshold was also used in gathering the set of trusted judgments for the evaluation (Section~\ref{subsec:expert_data}). All the data used in this chapter can be found in our data repository\footnote{\url{https://github.com/CrowdTruth/Cross-Task-Majority-Vote-Eval}}.


\subsection{Trusted Judgments Collection}
\label{subsec:expert_data}

To perform the evaluation, a set of trusted judgments is necessary to assess the correctness of crowd annotations. For each dataset, we manually evaluated the correctness of all the media unit annotations that were generated by the crowd and the experts. Depending on the task, the number of media unit-annotation pairs can become quite high, so we explored methods to make the manual evaluation more efficient.

For the datasets that contain expert annotation, we calculated the thresholds which yielded the maximum agreement in number of annotations between the crowd and expert annotations.  These annotations were then added to the trusted judgments collection, as the judgment in this case is unambiguous.  The interesting cases appear when crowd and expert disagree.  Previous work we performed in crowdsourcing \textit{Medical Relation Extraction}~\cite{aroyo2015truth} has indicated that experts might not always provide better annotations than crowd workers.  Additionally, for the \textit{Sound Interpretation} task we noticed that experts provided considerably fewer tags than the crowd, and there was a large discrepancy between annotations of crowds and experts, with a very small overlap between their annotations.  Therefore, instead of simply relying on expert judgment, the annotations where crowd and expert disagree were manually relabeled by exactly one of the authors, and then added to the trusted judgments set, which is also published in our data repository. In Appendix~\ref{sec:appendix} we present a selection of examples where the expert judgment is different from the trusted judgment. While these cases might call into question the level of expertise of the domain experts, inconsistencies and disagreement in expert annotation are regularly reported in various annotation tasks~\cite{cheatham2014conference,mcdonnell2016relevant,INEL16.635}.  Furthermore, in Section~\ref{sec:results} we will show that using the trusted judgments for evaluation still results in the expert performing the best for 2 out of 3 tasks. The only task where the expert underperforms is \textit{Sound Interpretation}, where the set of annotations provided by the expert is much smaller than the one provided by the crowd.


\begin{figure}[!b]
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/medical_mv.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/tweets_mv_2.png}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/events_mv.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/sounds_mv_2.png}
\end{subfigure}
\caption{CrowdTruth F1 scores for all crowdsourcing tasks.}
\label{fig:f1_mv}
\end{figure}

We collected expert annotations for the \textit{Medical Relation Extraction} data by employing medical students. Each sentence was annotated by exactly one person. The annotation task consisted of deciding whether or not the UMLS seed relation discovered by distant supervision is present in the sentence for the two selected terms.

For the \textit{Sound Interpretation} task, each sound in the dataset contains a description and a set of keywords that were provided by the authors of the sounds. We consider the keywords provided by the sounds' authors as trusted judgments given by domain experts.

The \emph{news event extraction} data was annotated with events by various linguistic experts. In total, 5 people annotated each sentence but we only have access to the final annotations, a consensus among the annotators. In the annotation guidelines described in \cite{pustejovsky2003timebank}, events are defined as situations that happen or occur, but are not generic situations. In contrast to the crowdsourcing task, where the workers had very loose instructions, the experts had very strict rules for identifying events, strictly based on linguistic features: \emph{(i)} tensed verbs: has called, will leave, was captured, \emph{(ii)} stative adjectives: sunken, stalled, on board and \emph{(iii)} event nominals: merger, Military Operation, Gulf War.

The only task without expert annotation is \textit{Twitter Event Identification} -- as it is in the open domain, no experts exist for this type of data.

\section{Results}
\label{sec:results}

We begin by evaluating \textit{how the majority vote method compares with CrowdTruth}, by calculating the precision/recall metrics using the gold standards we collected for each of the four crowdsourcing tasks.  Figure~\ref{fig:f1_mv} shows the F1 score for CrowdTruth over the four tasks.  The results are calculated for different media unit-annotation score thresholds for separating the data points into positive and negative classifications.  Table~\ref{tab:f1_mv} shows the detailed scores for CrowdTruth, given the highest F1 media unit-annotation score threshold.

\begin{table}[!tb]
\centering
\scalebox{0.8}{
\begin{tabular}{lrccccl}
\toprule
\textsc{Task} & \textsc{Dataset} & \textsc{Precision} & \textsc{Recall} & \textsc{F1 score} & \textsc{Accuracy} &  \textsc{Threshold} \\ \toprule

\multirow{4}{*}{\parbox{1.8cm}{\textsc{Medical \\ Relation \\ Extraction}}} & CrowdTruth & 0.86 & 0.962 & 0.908 & 0.932 & \multirow{4}{*}{0.6} \\
 & expert & 0.899 & 0.89 & 0.895 & 0.927 &  \\
 & majority vote & 0.924 & 0.781 & 0.847 & 0.902 & \\
 & single & 0.222 & 0.776 & 0.346 & 0.748 & \\ \hline

\multirow{3}{*}{\parbox{1.8cm}{\textsc{Twitter \\ Event \\ Identification}}} & CrowdTruth & 0.965 & 0.945 & 0.955 & 0.995 & \multirow{3}{*}{0.4} \\
 & majority vote & 0.984 & 0.885 & 0.932 & 0.984 & \\
 & single & 0.959 & 0.819 & 0.884 & 0.972 & \\ \hline

\multirow{4}{*}{\parbox{1.8cm}{\textsc{News \\ Event \\ Extraction}}} & CrowdTruth & 0.984 & 0.929 & 0.956 & 0.931 & \multirow{4}{*}{0.05} \\
 & expert & 0.983 & 0.944 & 0.963 & 0.942 & \\
 & majority vote & 0.985 & 0.375 & 0.544 & 0.492 & \\
 & single & 0.99 & 0.384 & 0.554 & 0.501 & \\ \hline

\multirow{4}{*}{\parbox{1.8cm}{\textsc{Sound \\ Inter- \\ pretation}}} & CrowdTruth & 1 & 0.729 & 0.843 & 0.815 & \multirow{4}{*}{0.1} \\
 & expert & 1 & 0.291 & 0.45 & 0.515 & \\
 & majority vote & 1 & 0.148 & 0.258 & 0.418 & \\
 & single & 1 & 0.098 & 0.178 & 0.383 & \\ 
\bottomrule
\end{tabular}
}
\caption {CrowdTruth evaluation results; the ``Threshold'' column shows the highest F1 media unit - annotation score threshold for each task, for which the evaluation was done.}
\label{tab:f1_mv}
\end{table}

\begin{table}
\centering
\scalebox{0.8}{
\begin{tabular}{rccc}
\toprule
\textsc{Task} & \textsc{Maj. Vote} & \textsc{Expert} & \textsc{Single} \\ \toprule
Medical Relation Extraction & $0.0001$ & $0.629$ & $< 2.2 \times 10^{-16}$ \\ 
Twitter Event Identification & $0.0001$ & N/A &  $6.145 \times 10^{-15}$ \\ 
News Event Extraction &  $< 2.2 \times 10^{-16}$ & $0.505$ & $< 2.2 \times 10^{-16}$ \\
Sound Interpretation & $< 2.2 \times 10^{-16}$ & $< 2.2 \times 10^{-16}$ & $< 2.2 \times 10^{-16}$ \\ %\hline
\bottomrule
\end{tabular}
}
\caption{$p$-values for McNemar's test of statistical significance in the CrowdTruth classification, compared with the others.}
\label{tab:stat_sig}
\end{table}

Across all four tasks, the CrowdTruth method performs better than both majority vote and the single annotator dataset.  While majority vote unsurprisingly performs the best on precision, as a consequence of its lower rate of positive labels, CrowdTruth consistently scores the best for both recall, F1 score and accuracy.  These differences in classification are statistically significant, as shown in Table~\ref{tab:stat_sig} -- this was calculated using McNemar's test~\cite{mcnemar1947note} over paired nominal data.

The evaluation of CrowdTruth compared with the expert is more nuanced. For the \textit{Medical Relation Extraction} and \textit{news event extraction tasks}, CrowdTruth performs as well as the expert annotators, with p-values indicating there is no statistically significant difference in the classifications.  In contrast, for the task of \textit{Sound Interpretation}, CrowdTruth performs better than the expert by a large margin.

\begin{figure}[!tb]
\centering
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/medical_workf1_2.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/tweets_workf1_2.png}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/events_workf1_2.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/sounds_workf1_2.png}
\end{subfigure}
\caption{The effect of the number of workers per unit on the F1 score, calculated at the best media unit-annotation score threshold (Table~\ref{tab:f1_mv}). For every point, the F1 is calculated with at most the given number of workers. The number of units used in the calculation of the F1 is shown in the y-axis on the right.}
\label{fig:f1_workers}
\end{figure}

The second evaluation shows the \textit{influence of the number of workers on the quality of the CrowdTruth data}.  Figure~\ref{fig:f1_workers} shows the CrowdTruth F1 score in relation to the number of workers.  Given one task, the number of workers per unit varies because of spam removal, so the F1 score was calculated using at most the number of workers at every point in the graph. The number of units annotated with the given number of workers is also shown in the graph.

The effects of the number of workers on the CrowdTruth F1 is clear -- more workers invariably leads to a higher F1 score.  For the tasks of \textit{Medical Relation Extraction}, \textit{Twitter Event Identification} and \textit{News Event Extraction}, the CrowdTruth F1 grows into a straight line, showing that the opinions of the crowd stabilize after enough workers.  For the \textit{Sound Interpretation} task, the CrowdTruth F1 score is still on an upwards trend after 10 workers, possibly indicating that more workers are necessary to get the full spectrum of annotations.

Figure~\ref{fig:f1_workers} also shows that CrowdTruth performs better than majority vote regardless of the number of workers per task. For closed tasks, increasing the number of workers has a positive impact on the majority vote F1 score.  For open tasks, adding more workers has less of an effect -- more workers increase the size of the annotation set for a unit, which is typically larger than for closed tasks, but the agreement is low because opinions are split between possible annotations.

\begin{figure}[!tbh]
\centering
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/exp_eval_relex.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
%\includegraphics[width=\linewidth]{img/exp_eval_events.png}}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/exp_eval_events.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{img/exp_eval_sounds.png}
\end{subfigure}
\caption{CrowdTruth F1 score evaluation, using expert annotation as ground truth.}
\label{fig:f1_exp}
\end{figure}

Finally, Figure~\ref{fig:f1_exp} shows an evaluation of CrowdTruth using only the expert annotations as ground truth (the \textit{Twitter Event Identification} task does not have experts, so it could not be evaluated). The F1 scores are lower than in the evaluation over the trusted judgments collection. For the \textit{Medical Relation Extraction Task}, majority vote performs essentially the same as CrowdTruth, whereas for the open-ended tasks, CrowdTruth still performs better. However, as we have shown in the Appendix, the expert annotations contain errors and are sometimes incomplete, particularly in the case of open-ended tasks. The evaluation using expert ground truth was done to show that the trusted judgments set is not biased in favor of CrowdTruth.


\section{Discussion}
\label{sec:discussion}

This chapter discusses the two main findings of the experiments: (1) that the ambiguity-aware CrowdTruth approach with multiple annotators and disagreement-based quality scores can perform better than majority vote, and (2) that increasing the number of workers has a significant impact on the quality of CrowdTruth annotations.

\subsection{CrowdTruth vs. Majority Vote}


The first goal in this chapter was to show that the ambiguity-aware \textbf{CrowdTruth approach performs better than majority vote}, a method that enforces consensus among annotators.  Our results over several crowdsourcing tasks, as seen in Figure~\ref{fig:f1_mv}, show this clearly. The gap in performance between CrowdTruth and majority vote is the most striking for open tasks (\textit{News Event Extraction} and \textit{Sound Interpretation}).  These tasks also require the lowest agreement threshold for achieving the best performance with CrowdTruth.  During the trusted judgments collection process, we observed how these tasks are prone to a wide range of opinions -- for instance, in the case of \textit{Sound Interpretation}, there are frequent examples of labels that are semantically dissimilar, but could reasonably be applied to the same sound (e.g. the same sound was annotated with the tag \texttt{balloon popping} by one worker, and with \texttt{gunshot} by another worker).  Because of this, enforcing consensus does not work for these tasks, and ambiguity-aware annotation aggregation appeared to be a viable solution.

Our evaluation also shows that \textbf{processing crowd data with ambiguity-aware metrics performs at least as well as expert annotators}, which is not the case for majority vote.  Crowdsourcing annotation is significantly cheaper in cost than experts --  e.g. even with 15 workers per unit, crowdsourcing for the task of \textit{Medical Relation Extraction} cost 2/3 of what the experts did. The crowd also has the advantage of being readily available on platforms such as Figure Eight, while the process of finding and hiring expert annotators can incur significant time costs. As our results showed, in order for the crowdsourcing to produce results comparable in quality to that of experts, appropriate processing with ambiguity-aware metrics is a necessity.

The variation in the optimal media unit-annotation score thresholds across the tasks shows that \textbf{the level of ambiguity is dependent on the crowdsourcing task}, thus supporting our triangle of disagreement model (Section~\ref{subsec:metrics}).  It is not surprising that the task with the highest agreement threshold (\textit{Medical Relation Extraction}) also has the most exact definition of a correct answer (i.e. whether a medical relation is expressed or not in a given sentence).  The definition of a medical relation is fairly clear; in contrast, the definition of an event is more subjective, therefore workers were able to come up with a wider range of correct annotations.

The experimental setup provides an empirical method for selecting the optimal threshold for media unit-annotation score.  However, if performing an evaluation with trusted judgments is not possible, selecting the optimal threshold becomes more difficult.  For open-ended tasks, the experiments indicate that almost all opinions matter, and the agreement threshold should be as low as possible.  In these cases, spam workers can be successfully eliminated by in-task effort consistency checks, and there is no need to enforce agreement beyond that.  In contrast, the experiments for closed tasks show higher agreement thresholds tend to work better.  The difficulty as well as the subjectivity of the domain also appear to have an impact.  The threshold should grow together with the difficulty, and inversely with subjectivity.  However, both difficulty and subjectivity might be difficult to measure in practice.  In the end, the tuning of the threshold should be regarded similarly to a precision-recall trade-off analysis, where the optimal value depends on the requirements of the ground truth (high precision but many false negative crowd labels, or high recall but more false positives).  The high variability for optimal threshold values also shows the limitations of traditional evaluation metrics like precision and recall that rely on discrete labels.  CrowdTruth metrics were constructed to measure ambiguity on a continuous scale, but the use of standard metrics resulted in losing this information by forcing the conversion to either positive or negative. Ultimately, our goal is to move away from a binary ground truth that needs to be calculated using a fixed threshold, and instead to use the CrowdTruth metrics to express ambiguity on a continuous scale.


\subsection{Finding the Right Number of Workers}


The second goal of the experiment was to show \textbf{increasing the number of workers improves the quality of CrowdTruth} annotations.  The results in Figure~\ref{fig:f1_workers} clearly show the increase in F1 score for CrowdTruth as more workers contribute to the tasks. This combined with the poor performance of the single annotator dataset proves the importance in considering a large enough pool of workers to be able to accurately capture the full spectrum of opinions.

The stabilization of the F1 score for \textit{Medical Relation Extraction}, \textit{Twitter Event Identification} and \textit{News Event Extraction} is an indication that we have indeed managed to collect the entire set of opinions for these tasks.  The fact that the scores all stabilize at different points in the graph (around 8 workers for \textit{Medical Relation Extraction}, 5 for \textit{Twitter Event Identification}, and 10 for \textit{News Event Extraction}) indicates that the \textbf{optimal number of workers is dependent on the task type}, thus also confirming our hypothesis that more workers than what is typically being considered in crowdsourcing studies are necessary for acquiring a high quality ground truth.

There exists a trade-off between cost and quality of annotations that should also be considered when optimizing the number of workers.  The higher cost was justified for these tasks, as the expert annotation was three times more expensive than the crowdsourced annotations at expert quality level.

An interesting observation is that the optimal number of workers per task does not seem to influence the optimal media unit-annotation score threshold for the task.  The \textit{News Event Extraction} requires a high number of workers, but the optimal media unit-annotation score threshold is low, while the \textit{Twitter Event Identification} requires a low number of workers, and also a low media unit-annotation score threshold, at least compared to \textit{Medical Relation Extraction}. 

While four tasks is a small sample to draw conclusions from, our findings seem to indicate that ambiguity in the crowdsourcing system has an impact on both the optimal number of workers per task, as well as the clarity of the media units.  These observations will form the basis for our future research in modeling crowd disagreement.

Finally, it is worth discussing the outlier characteristics of the \textit{Sound Interpretation} task. It is the only task that does not achieve a stable F1 curve (Figure~\ref{fig:f1_workers}) possibly due to insufficient workers assigned to it. It is also unique in its lack of false positive examples -- precision is 1 for the optimal media unit-annotation score threshold (Table~\ref{tab:f1_mv}), meaning that all labels collected from the crowd were accepted as part of the trusted judgments, with the exception of the spam workers that were removed from the set.  \textit{Sound Interpretation} is also the only task for which the expert annotator performed comparatively poor, with a statistically significant difference from CrowdTruth. As mentioned in the beginning of this section, after collecting the trusted judgments for this task, it became clear that the main challenge for the \textit{Sound Interpretation} task is not to achieve consensus between annotators, but to collect the entire spectrum of annotations that describe a sound, given that this spectrum is so large (e.g. the tags \texttt{balloon popping} and \texttt{gunshot} can both reasonably apply to the same sound). For this reason, it was difficult to label tags as false positives, and the annotations of the workers, experts included, were largely non-overlapping, as they tended to interpret the sounds quite differently.  The \textit{Sound Interpretation} task is therefore an extreme example of subjective ground truth. \\


\section{Related Work}
\label{sec:relatedwork}

\subsection{Crowdsourcing Ground Truth}

Crowdsourcing has grown into a viable alternative to expert ground truth collection, as crowdsourcing tends to be both cheaper and more readily available than domain experts. Experiments have been carried out in a variety of tasks and domains:  medical entity extraction~\cite{zhai2013web,Finin2010,van2012eu}, medical relation extraction~\cite{kilicoglu2011constructing,van2012eu}, open-domain relation extraction~\cite{kondreddi2014combining}, clustering and disambiguation~\cite{Lee2013}, ontology evaluation~\cite{noy2013mechanical}, web resource classification~\cite{castano2016human} and taxonomy creation~\cite{bragg2013crowdsourcing}. \cite{Snow2008} have shown that aggregating the answers of an increasing number of unskilled crowd workers with majority vote can lead to high quality NLP training data. The typical approach in these works is to assume the existence of a universal ground truth. Therefore, disagreement between annotators is considered an undesirable feature, and is usually discarded by using either of the following methods: restricting annotator guidelines, picking one answer that reflects some consensus usually through majority voting, or using a small number of annotators.


\subsection{Disagreement \& Ambiguity in Crowdsourcing}

Besides CrowdTruth, there exists some research on how disagreement in crowdsourcing should be interpreted and handled. In assessing the OAEI benchmark, \cite{cheatham2014conference} found that disagreement between annotators (both crowd and expert) is an indicator for inherent uncertainty in the domain knowledge, and that current benchmarks in ontology alignment and evaluation are not designed to model this uncertainty. \cite{plank-hovy-sogaard:2014:P14-2} found similar results for the task of crowdsourced part-of-speech tagging -- most inter-annotator disagreement was indicative of debatable cases in linguistic theory, rather than faulty annotation. \cite{Bayerl2011} also investigate the role of inter-annotator disagreement as a possible indicator of ambiguity inherent in natural language. \cite{lau2014measuring} propose a method for crowdsourcing ambiguity in the grammatical correctness of text by giving workers the possibility to pick various degrees of correctness, but inter-annotator disagreement is not discussed as a factor in measuring this ambiguity. \cite{schaekermann2016} propose a framework for dealing with uncertainty in ground truth that acknowledges the notion of ambiguity, and uses disagreement in crowdsourcing for modeling this ambiguity. For the task of word sense disambiguation, \cite{jurgens2013embracing} show that, in modeling ambiguity, the crowd was able to achieve expert-level quality of annotations. \cite{Chang:2017:Revolt} implemented a workflow of tasks for collecting and correcting labels for text and images, and found that ambiguous cases cannot simply be resolved by better annotation guidelines or through worker quality control. Finally, \cite{lin2014re} shows that often, machine learning classifiers can achieve a higher accuracy when trained with noisy crowdsourcing data. To our knowledge, this chapter presents the first experiment across several tasks and domains that explores ambiguity as a property of crowdsourcing systems, and how it can be interpreted to improve the quality of ground truth data.


\subsection{Crowd Aggregation beyond Majority Vote}

The literature on alternative crowdsourcing aggregation metrics typically focuses on analyzing worker performance -- identifying spam workers~\cite{Bozzon:2013,Kittur2008,Ipeirotis:2010}, and analyzing workers' performance for quality control and optimization of the crowdsourcing processes~\cite{Singer:2013}. \cite{NIPS2009_3644} and \cite{welinder2010multidimensional} have used a latent variable model for task difficulty, as well as latent variables to measure the skill of each annotator, to optimize crowdsourcing for image labels. \cite{werling2015job} use on-the-job learning with Bayesian decision theory to assign the most appropriate workers for each task, for both text and image annotation. Finally, \cite{prelec2017solution} show that the surprisingly popular crowd choice (i.e. the answer that most workers thought would not be picked by other workers, even though it is correct) gave better results than the majority vote for a variety of tasks with unambiguous ground truths (state capitals, trivia questions and price of artworks).

All of these approaches show promising improvements over the use of majority vote as an aggregating method.  These methods were developed only for closed tasks, primarily dealing with classification.  However, the novel approach of CrowdTruth allows to explore both closed and open-ended tasks.  Furthermore, our focus is on modeling ambiguity as a latent variable in the crowdsourcing system, as well as its role in generating inter-annotator disagreement, which these approaches currently do not take into account. We believe an optimal crowdsourcing approach would combine both ambiguity modeling, as well as specialized task assignment to workers. For instance, \cite{felt2015early} developed a generative model to aggregate crowd scores that incorporates features of the data (e.g. number of words), although they do not evaluate the performance of specific features. Ambiguity as measured with CrowdTruth, like the media unit-annotation score, could be used as a data feature in such a system.


\section{Conclusions}
\label{sec:conclusions}

Gathering human annotation is a major bottleneck in the process of knowledge base curation. Crowdsourcing-based approaches are gaining popularity in the attempt to solve the issues related to volume of data and lack of annotators. Typically these practices use inter-annotator agreement as a measure of quality. However, by ignoring inter-annotator disagreement, these practices tend to create artificial data that is neither general nor reflects the ambiguity inherent in the source.

In this chapter, we investigated what is the impact of inter-annotator disagreement on the quality of data across a variety of crowdsourcing tasks. To capture inter-worker disagreement, we presented an empirically derived methodology for efficiently gathering of human annotation by aggregating crowdsourcing data with CrowdTruth metrics, which harness the inter-annotator disagreement. We applied this methodology over a set of diverse crowdsourcing tasks: closed tasks (\textit{Medical Relation Extraction}, \textit{Twitter Event Identification}), and open-ended tasks (\textit{News Event Extraction} and \textit{Sound Interpretation}).

 Our results showed that \textit{preserving disagreement in the annotations allows us to collect richer data}, which enables reasoning about the ambiguity of the content being annotated. This is intrinsically relevant to the Semantic Web community, i.e. to identify the semantics of ambiguity across all modalities, e.g. text, images, videos and sounds. In all the tasks we considered, ambiguity-aware quality scores provide better ground truth data than the traditional majority vote. Moreover, we have shown that CrowdTruth annotations have at least the same quality, even better in the case of \textit{Sound Interpretation}, as expert annotations. Finally, we showed that, contrary to the common crowdsourcing practice of employing a small number of annotators, adding more crowd workers actually can lead to significantly better annotation quality.

In the future, we plan to expand our methodology to more complex annotation tasks, that require multiple or combined types of input beyond the closed/open-ended categorization we presented in this chapter. We are also working on expanding the CrowdTruth metrics for ambiguity to incorporate the state-of-the art in modeling crowd worker and data features~\cite{felt2015early}. Finally, we want to use the CrowdTruth data in practice for training and evaluating information extraction models used to populate the Semantic Web.



\section*{Acknowledgements}

We would like to thank Emiel van Miltenburg for assisting with the exploration of feature analysis of sounds, Chang Wang and Anthony Levas for providing and assisting with the medical data, Zhaochun Ren for the help in gathering the Twitter dataset, Tommaso Caselli for providing the news dataset, and the anonymous crowd workers for their contributions to our crowdsourcing tasks.

\newpage

%\section{Appendix: Example Media Units Where the Expert Judgment Is Different from the Trusted Judgment}
\section{Appendix: Dataset Examples}
\label{sec:appendix}

\begin{table}[!htb]
\centering
\scalebox{0.8}{
\begin{tabular}{p{6cm}cccc}
\toprule
\multirow{2}{*}{\textsc{Media Unit}} & \multirow{2}{*}{\textsc{Annotation}} & \textsc{Expert} & \textsc{Crowd} & \textsc{Trusted} \\ 
 & & \textsc{Judgment} & \textsc{Score} & \textsc{Judgment} \\ \toprule
 
The \textbf{epidermal nevus syndrome} is a neurocutaneous disorder characterized by \textbf{distinctive skin lesions} and often serious somatic and central nervous system (CNS) abnormalities. &	\multirow{6}{*}{$cause$} & \multirow{6}{*}{no} & \multirow{6}{*}{0.98} & \multirow{6}{*}{yes} \\ %\hline

\cellcolor{aliceblue}For empiric $treat$ment of epididymitis, especially when gonococcal or \textbf{chlamydial infection} is likely Ofloxacin or \textbf{levofloxacin} should be used  only  if epididymitis is not $cause$d by gonorrhea. & \cellcolor{aliceblue}\multirow{6}{*}{$treat$} & \cellcolor{aliceblue}\multirow{6}{*}{no} & \cellcolor{aliceblue}\multirow{6}{*}{0.966} & \cellcolor{aliceblue}\multirow{6}{*}{yes} \\ %\hline 

In contrast, we did not find a definite increase in the LGL percentage within 6 months postpartum in patients with \textbf{Graves' disease} who relapsed into \textbf{Graves' thyrotoxicosis}. & \multirow{6}{*}{$cause$} & \multirow{6}{*}{no} & \multirow{6}{*}{0.738} & \multirow{6}{*}{yes} \\ %\hline 

\cellcolor{aliceblue}The 1 placebo controlled trial that found black cohosh to be effective for \textbf{hot flashes} did not find \textbf{estrogen} to be effective, which casts doubt on the study's validity. & \cellcolor{aliceblue}\multirow{5}{*}{$treat$} & \cellcolor{aliceblue}\multirow{5}{*}{no} & \cellcolor{aliceblue}\multirow{5}{*}{0.73} & \cellcolor{aliceblue}\multirow{5}{*}{yes} \\ %\hline 

\textbf{Multicentric reticulohistiocytosis (MR)} is a \textbf{systemic disease} of unknown $cause$ characterized by the presence of a heavy macrophage infiltrate in skin and synovial tissues and the development of an erosive polyarthritis. & \multirow{8}{*}{$cause$} & \multirow{8}{*}{yes} & \multirow{8}{*}{0.697} & \multirow{8}{*}{no} \\ %\hline 

\cellcolor{aliceblue}Urokise versus \textbf{tissue plasminogen activator} in \textbf{pulmonary embolism}. & \cellcolor{aliceblue}\multirow{2}{*}{$treat$} & \cellcolor{aliceblue}\multirow{2}{*}{yes} & \cellcolor{aliceblue}\multirow{2}{*}{0.365} & \cellcolor{aliceblue}\multirow{2}{*}{no} \\ %\hline 

The principal differences between these vaccines are the transmission of live vaccine viruses from recipients to their contacts and the occurrence of occasional cases of \textbf{paralytic poliomyelitis} associated with use of \textbf{live poliovirus vaccine} & \multirow{7}{*}{$treat$} & \multirow{7}{*}{yes} & \multirow{7}{*}{0.1} & \multirow{7}{*}{no} \\ %\hline 

\cellcolor{aliceblue}These cases highlight the importance of considering \textbf{PTLD} in the differential diagnosis of \textbf{lymphadenopathy}. & \cellcolor{aliceblue}\multirow{4}{*}{$cause$} & \cellcolor{aliceblue}\multirow{4}{*}{yes} & \cellcolor{aliceblue}\multirow{4}{*}{0.09} & \cellcolor{aliceblue}\multirow{4}{*}{no} \\ %\hline 
\bottomrule
\end{tabular}
}
\caption {Example sentences from the \textit{Medical Relation Extraction} task where the expert judgment is different from the trusted judgment. The pair of terms that express the medical relation are shown in italic font in the media unit.}
\label{tab:ex_relex}
\end{table}

\begin{table}[!hb]
\centering

\scalebox{0.8}{
\begin{tabular}{p{6cm}cccc}
\toprule
\multirow{2}{*}{\textsc{Media Unit}} & \multirow{2}{*}{\textsc{Annotation}} & \textsc{Expert} & \textsc{Crowd} & \textsc{Trusted} \\ 
 & & \textsc{Judgment} & \textsc{Score} & \textsc{Judgment} \\ \toprule
 
The plan provides for the \textbf{distribution} of one common stock-purchase right as a dividend for each share of common outstanding & \multirow{4}{*}{$distribution$} & \multirow{4}{*}{no} & \multirow{4}{*}{0.95} & \multirow{4}{*}{yes} \\ %\hline 

\cellcolor{aliceblue}Two Middle East terrorists with records of successful \textbf{attacks} against Western targets Abu Nidal and Abu Abbas have ties to Baghdad. & \cellcolor{aliceblue}\multirow{4}{*}{$attacks$} & \cellcolor{aliceblue}\multirow{4}{*}{no} & \cellcolor{aliceblue}\multirow{4}{*}{0.73} & \cellcolor{aliceblue}\multirow{4}{*}{yes} \\ %\hline 

Secretary of State James Baker said on ABC-TV's ``This Week With David Brinkley'' that the series of UN resolutions condemning Iraq's \textbf{invasion} of Kuwait ``imply that the restoration of peace and stability in the Gulf would be a heck of a lot easier if he and that leadership were not in power in Iraq.'' & \multirow{9}{*}{$invasion$} & \multirow{9}{*}{no} & \multirow{9}{*}{0.53} & \multirow{9}{*}{yes} \\ %\hline 

\cellcolor{aliceblue}The company also said it continues to explore all options concerning the possible \textbf{sale} of National Aluminum's 54.5\% stake in an aluminum smelter in Hawesville Ky. & \cellcolor{aliceblue}\multirow{5}{*}{$sale$} & \cellcolor{aliceblue}\multirow{5}{*}{no} & \cellcolor{aliceblue}\multirow{5}{*}{0.24} & \cellcolor{aliceblue}\multirow{5}{*}{yes} \\ %\hline 

Yield on the issue was 7.88\% & $no$ $event$ & yes & 0.14 & no \\ %\hline 

\cellcolor{aliceblue}Har-Shefi said she heard Amir talk about killing Rabin but did not tell the police because she did not believe he was \textbf{serious}. & \cellcolor{aliceblue}\multirow{4}{*}{$serious$} & \cellcolor{aliceblue}\multirow{4}{*}{yes} & \cellcolor{aliceblue}\multirow{4}{*}{0} & \cellcolor{aliceblue}\multirow{4}{*}{no} \\ %\hline 

The American hope is that someone from within Iraq perhaps from the army 's professional ranks will step forward and push Saddam Hussein aside so that the country can begin recovering from the disaster. & \multirow{6}{*}{$no$ $event$} & \multirow{6}{*}{yes} & \multirow{6}{*}{0} & \multirow{6}{*}{no} \\ %\hline
\bottomrule
\end{tabular}
}

\caption {Example sentences from the \textit{News Event Extraction} task where the expert judgment is different from the trusted judgment. The annotation is shown in italic font in the media unit.}
\label{tab:ex_news}
\end{table}


\begin{table}[!t]
\centering
\scalebox{0.8}{
\begin{tabular}{p{4cm}ccccc}
\toprule
\multirow{2}{*}{\textsc{Media Unit URL}} & \textsc{Media Unit} & \multirow{2}{*}{\textsc{Annotation}} & \textsc{Expert} & \textsc{Crowd} & \textsc{Trusted} \\ 
 & \textsc{Description} &  & \textsc{Judgment} & \textsc{Score} & \textsc{Judgment} \\ \toprule

\multirow{4}{4cm}{\url{https://freesound.org/data/previews/21/21266_88803-hq.mp3}} & \multirow{4}{*}{jazz} & cymbals & no & 0.272 & yes \\ \cline{3-6}
& & bangle & no & 0.136 & yes \\ \cline{3-6}
& & rhythmic & no & 0.136 & yes \\
& &  &  & &  \\ \hline

\multirow{4}{4cm}{\url{https://freesound.org/data/previews/26/26086_11477-hq.mp3}} & \multirow{4}{*}{chicken} & birds & no & 0.538 & yes \\ \cline{3-6}
& & geese & no & 0.359 & yes \\ \cline{3-6}
& & horns & no & 0.359 & yes \\ 
& &  &  & &  \\ \hline

\multirow{4}{4cm}{\url{https://freesound.org/data/previews/35/35823_317782-hq.mp3}} & \multirow{4}{*}{weird drums} & music & no & 0.875 & yes \\ \cline{3-6}
& & band & no & 0.145 & yes \\ \cline{3-6}
& & disco & no & 0.145 & yes \\
& &  &  & &  \\ \hline

\multirow{4}{4cm}{\url{https://freesound.org/data/previews/39/39329_404624-hq.mp3}} & \multirow{4}{*}{trip hop} & beat & no & 0.371 & yes \\ \cline{3-6}
& & percussion & no & 0.371 & yes \\ \cline{3-6}
& & chimes & no & 0.371 & yes \\ 
& &  &  & &  \\ \hline

\multirow{4}{4cm}{\url{https://freesound.org/data/previews/41/41462_78779-hq.mp3}} & \multirow{4}{*}{beer glasses} & clicks & no & 0.242 & yes \\ \cline{3-6}
& & clink & no & 0.242 & yes \\ \cline{3-6}
& & ding & no & 0.242 & yes \\
& &  &  & &  \\ %\hline
\bottomrule
\end{tabular}
}

\caption {Example sounds from the \textit{Sound Interpretation} task where the expert judgment is different from the trusted judgment.}
\label{tab:ex_sounds}
\end{table}
