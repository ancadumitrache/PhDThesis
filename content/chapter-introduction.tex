% !TEX root = ../thesis-example.tex
%
\chapter{Introduction}
\label{chap:intro}

%\epigraph{All that we see or seem \\ Is but a dream within a dream}{Edgar Allan Poe}{}

\begin{chapquote}{Antoine de Saint-Exup√©ry, \textsc{The Little Prince}}
Language is the source of misunderstandings.
\end{chapquote}

..

\section{Background \& Motivation}

..

\subsection{Crowd Data Quality}

Determining the quality of crowdsourced data collected from non-experts has been the subject of study since the work of \citet{Snow:2008}, who have shown that the crowd can produce annotations with expert-level quality for a variety of natural language processing tasks: affect  recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. Despite these promising results, establishing the quality of crowdsourced data is still a challenge, due primarily to the difficulty of identifying and preventing spam behavior of workers~\cite{difallah2012mechanical,demartini2012zencrowd}. This is especially the case when applying non-expert crowdsourcing to domains that are typically thought to require expertise on the part of the annotators.

The medical domain is particularly difficult, with even expert annotators sometimes producing low-quality data~\cite{Marcheggiani:2017:ELT:3139489.3106235}. Nevertheless, there exists some research that successfully employed non-expert crowdsourcing to collect annotations in the medical domain. \citet{mortensen2013crowdsourcing} use crowdsourcing to verify relation hierarchies in biomedical ontologies. On 14 relations from the SNOMED CT CORE Problem List Subset, the authors report the crowd's accuracy at 85\% for identifying whether the relations were correct or not. \citet{burger2012validating} used crowdsourcing to extract the gene-mutation relations in Medical Literature Analysis and Retrieval System Online (MEDLINE) abstracts. Focusing on a very specific gene-mutation domain, the authors report a weighted accuracy of 82\% over a corpus of 250 MEDLINE abstracts. \citet{li2015exposing} performed a study exposing ambiguities in a gold standard for drug-disease relations with crowdsourcing. They found that, over a corpus of 60 sentences, levels  of  crowd agreement varied in a similar manner to the levels of agreement  among  the  original  expert  annotators. \citet{zhai2013web} describe a method for crowdsourcing a ground truth for medical named entity recognition and entity linking. In a dataset of over 1,000 clinical trials, the authors show no statistically significant difference between the crowd and expert-generated gold standard for the task of extracting medications and their attributes.

Other difficult annotation tasks involve linguistics knowledge. For instance, frame disambiguation requires an understanding of the frame semantics theory~\cite{baker1998berkeley}, which can be difficult to explain to a crowd of non-experts. While \citet{Hong:2011:GCR:2018966.2018970} showed a high accuracy when comparing the crowd to experts for the task of frame disambiguation by simply calculating the majority vote, \citet{chang2015scaling} claim that a more complex multi-step annotation process is required in order to correct misunderstandings of the frame definition by the crowd.

In all of these experiments, disagreement between annotators is seen as undesirable and a sign of low quality data. In contrast, \citet{jurgens2013embracing} argues that ambiguity is an inherent feature of frame/word sense disambiguation, and that crowdsourcing can be used to capture it, by asking annotators to rate ambiguous examples on a Likert scale. Similarly, this thesis proposes that ambiguity is a useful property of natural language, but instead of asking workers directly to rate ambiguity, we study it through measuring inter-annotator disagreement. This presents an interesting challenge, as disagreement is usually removed from annotated datasets in order to improve their quality. Our goal in this work is to show that crowdsourced ground truth can still have quality comparable to that of domain experts, while still preserving the signals of worker disagreement.


\subsection{Crowdsourcing Aggregation Methods}

The most common way to aggregate crowd annotations is majority voting, where the label for an example is picked based on whether or not the majority of crowd workers agree that it exists. Inter-annotator agreement in crowdsourcing is usually employed as a method to determine the quality of the annotations. Typically, disagreement is considered an undesirable feature of the annotations -- a byproduct either of low quality of the workers, or of an unclear annotation task. There are several metrics to capture inter-annotator agreement, most popular being Cohen's $\kappa$~\cite{cohen1960kappa} and Krippendorff's $\alpha$~\cite{klaus2013content}. \citet{artstein2008inter} compared several of these metrics, finding that the choice of metric is not as important as it is to increase the number of annotators, in order to reduce the prevalence of personal bias.

In recent years, there is also a growing body of research on alternative crowdsourcing aggregation metrics. There is a particular focus on modeling the reliability of crowd workers, by identifying spam workers~\cite{Bozzon:2013,Kittur2008,Ipeirotis:2010}, and analyzing workers' performance for quality control and optimization of the crowdsourcing processes~\cite{Singer:2013}. \citet{NIPS2009_3644} and \citet{welinder2010multidimensional} have used a latent variable model for task difficulty, as well as latent variables to measure the skill of each annotator, to optimize crowdsourcing for image labels. \citet{werling2015job} use on-the-job learning with Bayesian decision theory to assign the most appropriate workers for each task, for both text and image annotation. \citet{prelec2017solution} show that the surprisingly popular crowd choice (i.e. the answer that most workers thought would not be picked by other workers, even though it is correct) gave better results than the majority vote for a variety of tasks with unambiguous ground truths (state capitals, trivia questions and price of artworks). Finally, \citet{paun2018comparing} compare majority vote with 6 different Bayesian methods that aggregate crowd results while also modeling worker reliability and task item difficulty. The evaluation over a variety of task settings (binary and multiple choice, different levels of quality for the workers) shows 5 out of 6 of the Bayesian methods consistently outperform majority vote.

Our research is part of this current trend of investigating the limitations of majority vote as a crowdsourcing aggregation method. The novel approach of CrowdTruth is the modeling of ambiguity as a latent variable of the crowdsourcing system, that is present in inter-worker disagreement. Therefore, instead of discarding it, the CrowdTruth approach preserves disagreement and uses it to identify ambiguous data points. In this thesis, we will show that the CrowdTruth method to aggregate crowdsourcing annotations is applicable to a variety of annotation tasks, where simply using majority vote would result in the loss of important information regarding the ambiguity in the data.


\subsection{Natural Language Processing with the Crowd}

Due to being both cheaper and more readily available than domain experts, crowdsourcing is used to collect ground truth for a variety of natural language processing tasks, across several domains: medical entity extraction~\cite{zhai2013web,Finin2010,van2012eu}, medical relation extraction~\cite{kilicoglu2011constructing,van2012eu}, open-domain relation extraction~\cite{kondreddi2014combining}, clustering and disambiguation~\cite{Lee2013}, ontology evaluation~\cite{noy2013mechanical}, web resource classification~\cite{castano2016human} and taxonomy creation~\cite{bragg2013crowdsourcing}. \citet{Snow2008} have shown that aggregating the answers of an increasing number of unskilled crowd workers with majority vote can lead to high quality natural language processing training data.

Relation extraction from text is a task that usually requires large amounts of training data, meaning that completely crowdsourcing the ground truth is cost-prohibitive. However, active and semi-supervised methods can be used to scale-up the signal in labeled data to unlabeled examples. \citet{angeli2014combining} used an active learning approach to identify candidate sentences for crowd labeling that will most impact the performance of their relation extraction model. \citet{levy2017zero} have shown that a small crowdsourced dataset of questions about relations can be exploited to perform zero-shot learning. \citet{pershina2014infusion} used a small dataset of hand-labeled data to generate relation-specific guidelines that are used as additional features in the relation extraction.

The approach in these works is to restrict disagreement between annotators by using either of the following methods: restricting annotator guidelines, picking one answer that reflects some consensus usually through majority voting, or using a small number of annotators. In this thesis, we explore the question of whether crowdsourced data that preserves disagreement can be used as ground truth for the task of relation classification in sentences. We investigate whether inter-annotator disagreement in particular is a useful signal that the relation classification model can learn from, and whether our crowdsourcing method can be scaled-up through a semi-supervised learning approach.


\subsection{Capturing Ambiguity}

Recent work in collecting annotations for text~\cite{poesio2005reliability,chang2016linguistic}, sounds~\cite{doi:10.1080/09298215.2016.1200631} and images~\cite{schaekermann2016,cheplygina2018crowd} found that disagreement between annotators is not just a result of poor quality work, and can actually be an indicator for other properties of the data, such as ambiguity and uncertainty~\cite{aroyo2018aimag}.

Our work is part of a continuous effort in exploring the inter-annotator disagreement as an indicator for (1) inherent uncertainty in the domain knowledge as \citet{cheatham2014conference} found when assessing the Ontology Alignment Evaluation Initiative (OAEI) benchmark, (2) debatable cases in linguistic theory, rather than faulty annotation, as \citet{plank-hovy-sogaard:2014:P14-2} found in their part-of-speech tagging task, and (3) ambiguity inherent in natural language~\cite{Bayerl2011}.

Finally we note recent efforts to consider in ground truth corpora (1) the notion of uncertainty, where \citet{schaekermann2016} also use disagreement in crowdsourcing for modeling it, (2) the notion of ambiguity, where \citet{Chang:2017:Revolt} found that ambiguous cases cannot simply be resolved by better annotation guidelines or through worker quality control, and (3) the notion of noise, where \citet{lin2014re} show that machine learning classifiers can often achieve a higher accuracy when trained with noisy crowdsourcing data.


\section{Crowdsourcing with CrowdTruth}

To address the challenge of capturing an interpreting disagreement in crowdsourced annotations, we have proposed the CrowdTruth methodology~\cite{aroyo2012harnessing,aroyo2013crowd}. The main goal of this methodology is to correct the myths of human annotation proposed by~\citet{aroyo2015truth}:

\begin{itemize}
    \item \textbf{Myth \#1: Disagreement is bad -} This myth holds that, whenever inter-annotator disagreement occurs, it is the fault of the annotators. Therefore disagreement should be eliminated at all cost. In contrast, CrowdTruth proposes the idea that disagreement can also be caused by ambiguity in the annotation task. Specifically for the case of natural language, ambiguity is an inherent property of text. CrowdTruth proposes that capturing disagreement is a method to capture this ambiguity.
    
    \item \textbf{Myth \#2: One is enough -} Following from the previous myth, if inter-annotator disagreement can be an important indicator, then having a single worker perform the annotations means missing out on the signal the disagreement can send. CrowdTruth proposes using multiple workers for the same annotation task, in order to collect all possible disagreement that can occur.
    
    \item \textbf{Myth \#3: Experts are better -} While traditional annotation tasks ask a single domain expert to give their input, CrowdTruth proposes expanding this singular perspective with that of crowd workers. That is not to say that the experts are not necessary, or that the crowd is better suited at understanding an annotation task, just that often the perspective of the crowd is necessary in order to get a more accurate overview.
    
    \item \textbf{Myth \#4: One truth -} This myth holds that, for each annotation task, there is only one possible truth to how it should be done. Instead of forcing annotations into binary labels that are either positive or negative, CrowdTruth proposes that the truth exists on a scale, and is expressed with various degrees of clarity.
    
    \item \textbf{Myth \#5: All examples are equal -} If the truth value of a label is expressed on a scale, then it follows that there exists some variability in how clear a label is expressed in different examples. For instance, for the task of text labeling, some texts might express the desired label more clearly than others. The CrowdTruth methodology attempts to capture the degree of clarity of each example in the annotation task through the use of dedicated metrics, thus quantifying the fact that not all examples are equal in how well they express a label.
    
    \item \textbf{Myth \#6: Detailed guidelines help -} Typical annotation practice which provides detailed annotation guidelines to workers, which result in brittle overly-specific interpretations of the annotations. In contrast, the CrowdTruth methodology employs simple annotation guidelines that allow for disagreement over the interpretation of annotations.
    
\end{itemize}


CrowdTruth metrics: v1~\cite{inel2013}, v2~\cite{dumitrache2018crowdtruth}


\section{Research Questions \& Summary of Chapters}

The overall goal of this thesis is to investigate the role of inter-annotator disagreement in crowdsourcing ground truth for natural language processing, as collected using CrowdTruth methodology and metrics. The main research goal is addressed by answering the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} \textit{How does disagreement-aware crowdsourcing compare with the expert annotator baseline?}
    
    Chapter~\ref{chap:med-rel-ex} explores this question for the task of medical relation extraction. In the medical domain it is typically assumed that expert annotators are required to get the best quality ground truth. This work shows that, by capturing the inter-annotator disagreement with the CrowdTruth method, medical relation classifiers trained on crowd annotations perform the same as those trained on expert annotations. Furthermore, classifiers trained on crowd annotations perform better than those trained with automatically-labeled data. Using the crowd also reduces the cost (monetary and in time required to find annotators) for collecting the data. This chapter is based on the following publication:
    
    \begin{itemize}
        \item Dumitrache, Anca, Lora Aroyo, and Chris Welty. ``Crowdsourcing ground truth for medical relation extraction.'' \textit{ACM Transactions on Interactive Intelligent Systems (TiiS)} 8.2 (2018): 12.~\cite{DBLP:journals/corr/DumitracheAW17}
    \end{itemize}

    \item \textbf{RQ2:} \textit{Is disagreement in crowdsourcing a useful property to capture? What is gained when aggregating crowdsourcing data with disagreement-aware metrics, as opposed to metrics that discard disagreement?}
    
    Chapter~\ref{chap:maj-vote} compares the performance of CrowdTruth metrics and majority vote, a consensus - enforcing metric, over a diverse set of crowdsourcing tasks. We show that applying the CrowdTruth methodology we collect richer data that allows us to reason about ambiguity of content. Furthermore, an increased number of crowd workers leads to growth and stabilization in the quality of annotations, going against the usual practice of employing a small number of annotators. This chapter is based on the following publication:
    
    \begin{itemize}
        \item Dumitrache, Anca, et al. ``Empirical methodology for crowdsourcing ground truth.'' \textit{Semantic Web Journal (in publication)}. 2018.~\cite{dumitracheempirical}
    \end{itemize}

    \item \textbf{RQ3:} \textit{How can natural language processing models learn from disagreement - aware crowdsourcing data?}
    
    In Chapter~\ref{chap:od-rel-ex} we discuss how CrowdTruth data can be used to better models for relation classification for sentences. We build on work from Chapter~\ref{chap:med-rel-ex}, where we have shown that training models on on crowd annotations gives better results than training with data automatically-labeled with distant supervision~\cite{mintz2009distant}. However, crowd data is expensive to collect. Chapter~\ref{chap:od-rel-ex} describes how to correct a large corpus of training data for relation classification, using a relatively small crowdsourced corpus, using two different methods: (1) by manually propagating the false positive and cross-relation signals identified with the help of the crowd, and (2) by adapting the semantic label propagation method~\cite{sterckx2016knowledge} to work with CrowdTruth data. This chapter is based on the following publications:
    
    \begin{itemize}
        \item Dumitrache, Anca, Lora Aroyo, and Chris Welty. ``False positive and cross-relation signals in distant supervision data.'' \textit{Proceedings of the Sixth Workshop on Automated Knowledge Base Construction (AKBC) at NIPS}. 2017.~\cite{dumitrache2017false}
        
        \item Dumitrache, Anca, Lora Aroyo, and Chris Welty. ``Crowdsourcing semantic label propagation in relation classification.'' \textit{Proceedings of the First Workshop on Fact Extraction and VERification (FEVER) at EMNLP}. 2018.~\cite{dumitrache2018crowdsourcing}
    \end{itemize}

    \item \textbf{RQ4:} \textit{Can disagreement in crowd annotations be used to capture the ambiguity inherent in natural language?}
    
    In Chapter~\ref{chap:frames}, we explore this question as applied to the task of disambiguating semantic frames (i.e. high-level concepts that represent the meanings of words). Similarly to Chapter~\ref{chap:med-rel-ex}, we show that the crowd achieves comparative quality with domain experts. A qualitative evaluation of cases when crowd and expert disagree shows that inter-annotator disagreement is an indicator of ambiguity in both frames and sentences. We demonstrate that the cases in which the crowd workers could not agree exhibit ambiguity, either in the sentence, frame, or the task itself, arguing that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating arbitrary targets for machine learning. This chapter is based on the following publication:
    
    \begin{itemize}
        \item Dumitrache, Anca, Lora Aroyo, and Chris Welty. ``Capturing ambiguity in crowdsourcing frame disambiguation.'' \textit{Proceedings of the Sixth AAAI Conference on Human Computation and Crowdsourcing (HCOMP)}. 2018.~\cite{DBLP:conf/hcomp/DumitracheAW18}
    \end{itemize}
\end{itemize}

In addition to the research question presented above, another contribution of this thesis is a collection of ground truth datasets for the tasks of medical relation extraction~\cite{anca_dumitrache_2016_50676}, open domain relation extraction~\cite{crowdODrelexdata2016}, and semantic frame disambiguation~\cite{anca_dumitrache_2018_1472345}. These datasets have been collected with crowdsourcing and processed with the CrowdTruth methodology. The disagreement-aware metrics have allowed us to label the data with continuous truth labels for sentences, relations, semantic frames and workers, allowing us to capture the ambiguity inherent in these tasks.