% !TEX root = ../thesis-example.tex
%
\chapter{Conclusion}
\label{sec:conclusion}

\begin{chapquote}{Simone de Beauvoir, \textsc{The Ethics of Ambiguity}}
Man must not attempt to dispel the ambiguity of his being but, on the contrary, accept the task of realizing it.
\end{chapquote}

\begin{chapquote}{Rainer Maria Rilke, \textsc{Letters to a Young Poet}}
\noindent
Have patience with everything that remains unsolved in your heart. \\
...live in the question.
\end{chapquote}

This chapter summarizes the research presented in this thesis, by revisiting the research questions from the introduction. We also discuss the limitations of the current work, and identify future research directions to extend and compliment our findings on how to handle disagreement in ground truth for natural language processing.

\section{Research Questions Revisited}

In this section, we consider again the research questions introduced at the beginning of this thesis. For each question, we provide possible answers, based on the research presented in this thesis. \\

\textbf{RQ1:} \textit{Does allowing disagreement in crowdsourcing ground truth yield the same quality as asking domain experts?}

In Chapter~\ref{chap:med-rel-ex}, we studied this research question for the task of medical relation extraction. Using the CrowdTruth methodology for disagreement-preserving crowdsourcing, we collected a gold standard of 3,984 sentences expressing medical relations, focusing on the $cause$ and $treat$ relations. This data was used to train a sentence-level classification model. We have shown that allowing the disagreement in the crowd data does not mean that the quality of the ground truth has to suffer -- the relation extraction models trained on crowd data performed just as well as the ones trained on annotations from medical experts, while the cost of collecting the data from the crowd was cheaper than for the experts.

In addition, our results show that, when the model reaches maximum performance after training, the crowd also performs better than distant supervision. Finally, we introduced and validated new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task. \\


\textbf{RQ2:} \textit{How does allowing disagreement in diverse crowdsourcing tasks influence the quality of the data?}

In Chapter~\ref{chap:maj-vote}, we studied the impact of inter-annotator disagreement on data quality for a set of diverse crowdsourcing tasks: closed tasks (\textit{Medical Relation Extraction}, \textit{Twitter Event Identification}), and open-ended tasks (\textit{News Event Extraction} and \textit{Sound Interpretation}). To do this, we employ an empirically derived methodology for efficiently gathering of human annotation by aggregating crowdsourcing data with CrowdTruth metrics, which harness the inter-annotator disagreement. Our results showed that preserving disagreement in the annotations allows us to collect richer data, which enables reasoning about the ambiguity of the content being annotated. In all the tasks we considered, ambiguity-aware quality scores provide better ground truth data than the traditional majority vote. Finally, we showed that, contrary to the common crowdsourcing practice of employing a small number of annotators, adding more crowd workers actually can lead to significantly better annotation quality. \\

%This is intrinsically relevant to the Semantic Web community, i.e. to identify the semantics of ambiguity across all modalities, e.g. text, images, videos and sounds. Our results also showed that, in all the tasks we considered, such ambiguity-aware quality scores provide better ground truth data than the traditional majority vote. Moreover, we have shown that CrowdTruth annotations have at least the same quality, even better in the case of \textit{Sound Interpretation}, as expert annotations.  Finally, we showed that, contrary to the common crowdsourcing practice of employing a small number of annotators, adding more crowd workers actually can lead to significantly better annotation quality.


\textbf{RQ3:} \textit{Can we improve the performance of natural language processing models by using disagreement-aware ground truth data?}

In Chapter~\ref{chap:od-rel-ex} we perform several experiments using disagreement-aware ground truth to train and evaluate models for open-domain relation classification in sentences. Using the crowd data as ground truth, we have shown a very significant variation in the false positive rate in distant supervision data, and it seems extremely likely that this can be exploited to improve training. An initial experiment showed that cross-relation signals that were identified by the crowd can be used correct training data for relation classification. Next, we explored a more robust approach that propagates human annotations to sentences that are similar in a low dimensional embedding space. We showed that a small crowdsourced dataset of 2,050 sentences, collected and aggregated with the disagreement-preserving CrowdTruth methodology, can be successfully used to correct training data labeled with distant supervision, using a technique called ``semantic label propagation''.  We have shown experimental results from training a relation classifier, where our method shows significant improvement over the distant supervision baseline, as well as just adding the labeled examples to the train set. Since the semantic label propagation is applied to the data before training is completed, this method can easily be reused to correct train data for other related models (e.g. to perform knowledge base completion), regardless of the features used in learning. \\


\textbf{RQ4:} \textit{Is inter-annotator disagreement an accurate indicator for ambiguity in natural language?}

In Chapter\ref{chap:frames}, we explore the relation between inter-annotator disagreement and natural language ambiguity for the task of frame disambiguation annotations in sentences. We performed an experiment over a set of 433 sentences annotated with frames from FrameNet corpus, and showed that the crowd annotations aggregated with disagreement-preserving CrowdTruth metrics are comparable in quality to domain experts -- the crowd achieves an F1 score greater than 0.67 compared to expert linguists, and an accuracy that is comparable to the state of the art~\cite{Hong:2011:GCR:2018966.2018970}.

We also showed cases where the crowd annotation is correct even though the expert is in disagreement, arguing for the need to have multiple annotators per sentence. Most importantly, we examined the cases in which crowd workers could not agree. We found that disagreement is caused by one or more of the following: workers misunderstanding the task, missing context from the sentences, frames with overlapping or abstract definitions. The results show a clear link between inter-annotator disagreement and ambiguity, either in the sentence, frame, or the task itself. We argue that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating brittle, incomplete datasets, and therefore arbitrary targets for machine learning.  We further argued that ranking examples by a score is informative, and that the crowd offers alternate interpretations that are often sensible.


\section{Limitations \& Future Directions}

The research presented in this thesis has several possible directions for future work. In addition to the limitations specific to the material in each chapter, we identify three overarching issues to be explored in the future work: (1) expanding the experimental work on capturing ground truth ambiguity beyond relation extraction and frame disambiguation, (2) optimizing for the cost of data collection, and (3) building natural language processing models that learn to recognize ambiguity.

\subsection{Beyond NLP/RelEx/Frames}

``Completeness proofs are notoriously difficult and are best avoided by any student who aims to finish his PhD on time.'' Judea Pearl in The Book of Why

compositionality of ambiguity

compare with more baselines


\subsection{The Cost of Ambiguity}

an incremental approach to crowdsourcing

gated crowd~\cite{liu2016effective} vs. CrowdTruth


\subsection{Learning Uncertainty}

it's difficult to learn from ambiguity because it's mostly outliers

automatically detecting ambiguity~\cite{lebanoff2018automatic}
