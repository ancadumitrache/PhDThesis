% !TEX root = ../thesis-example.tex
%
\chapter{Conclusion}
\label{sec:conclusion}

\begin{chapquote}{Simone de Beauvoir, \textsc{The Ethics of Ambiguity}}
Man must not attempt to dispel the ambiguity of his being but, on the contrary, accept the task of realizing it.
\end{chapquote}

\begin{chapquote}{Rainer Maria Rilke, \textsc{Letters to a Young Poet}}
\noindent
Have patience with everything that remains unsolved in your heart. \\
...live in the question.
\end{chapquote}

This chapter summarizes the research presented in this thesis, by revisiting the research questions from the introduction. We also discuss the limitations of the current work, and identify future research directions to extend and compliment our findings on how to handle disagreement in ground truth for natural language processing.

\section{Research Questions Revisited}

In this section, we consider again the research questions introduced at the beginning of this thesis. For each question, we provide possible answers, based on the research presented in this thesis. \\

\textbf{RQ1:} \textit{Does allowing disagreement in crowdsourcing ground truth yield the same quality as asking domain experts?}

In Chapter~\ref{chap:med-rel-ex}, we studied this research question for the task of medical relation extraction. Using the CrowdTruth methodology for disagreement-preserving crowdsourcing, we collected a gold standard of 3,984 sentences expressing medical relations, focusing on the $cause$ and $treat$ relations. This data was used to train a sentence-level classification model. We have shown that allowing the disagreement in the crowd data does not mean that the quality of the ground truth has to suffer -- the relation extraction models trained on crowd data performed just as well as the ones trained on annotations from medical experts, while the cost of collecting the data from the crowd was cheaper than for the experts.

In addition, our results show that, when the model reaches maximum performance after training, the crowd also performs better than distant supervision. Finally, we introduced and validated new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task. \\


\textbf{RQ2:} \textit{How does allowing disagreement in diverse crowdsourcing tasks influence the quality of the data?}

In Chapter~\ref{chap:maj-vote}, we studied the impact of inter-annotator disagreement on data quality for a set of diverse crowdsourcing tasks: closed tasks (\textit{Medical Relation Extraction}, \textit{Twitter Event Identification}), and open-ended tasks (\textit{News Event Extraction} and \textit{Sound Interpretation}). To do this, we employ an empirically derived methodology for efficiently gathering of human annotation by aggregating crowdsourcing data with CrowdTruth metrics, which harness the inter-annotator disagreement. Our results showed that preserving disagreement in the annotations allows us to collect richer data, which enables reasoning about the ambiguity of the content being annotated. In all the tasks we considered, ambiguity-aware quality scores provide better ground truth data than the traditional majority vote. Finally, we showed that, contrary to the common crowdsourcing practice of employing a small number of annotators, adding more crowd workers actually can lead to significantly better annotation quality. \\


\textbf{RQ3:} \textit{Can we improve the performance of natural language processing models by using disagreement-aware ground truth data?}

In Chapter~\ref{chap:od-rel-ex} we perform several experiments using disagreement-aware ground truth to train and evaluate models for open-domain relation classification in sentences. Using the crowd data as ground truth, we have shown a very significant variation in the false positive rate in distant supervision data, and it seems extremely likely that this can be exploited to improve training. An initial experiment showed that cross-relation signals that were identified by the crowd can be used correct training data for relation classification. Next, we explored a more robust approach that propagates human annotations to sentences that are similar in a low dimensional embedding space. We showed that a small crowdsourced dataset of 2,050 sentences, collected and aggregated with the disagreement-preserving CrowdTruth methodology, can be successfully used to correct training data labeled with distant supervision, using a technique called ``semantic label propagation''.  We have shown experimental results from training a relation classifier, where our method shows significant improvement over the distant supervision baseline, as well as just adding the labeled examples to the train set. Since the semantic label propagation is applied to the data before training is completed, this method can easily be reused to correct train data for other related models (e.g. to perform knowledge base completion), regardless of the features used in learning. \\


\textbf{RQ4:} \textit{Is inter-annotator disagreement an accurate indicator for ambiguity in natural language?}

In Chapter\ref{chap:frames}, we explore the relation between inter-annotator disagreement and natural language ambiguity for the task of frame disambiguation annotations in sentences. We performed an experiment over a set of 433 sentences annotated with frames from FrameNet corpus, and showed that the crowd annotations aggregated with disagreement-preserving CrowdTruth metrics are comparable in quality to domain experts -- the crowd achieves an F1 score greater than 0.67 compared to expert linguists, and an accuracy that is comparable to the state of the art~\cite{Hong:2011:GCR:2018966.2018970}.

We also showed cases where the crowd annotation is correct even though the expert is in disagreement, arguing for the need to have multiple annotators per sentence. Most importantly, we examined the cases in which crowd workers could not agree. We found that disagreement is caused by one or more of the following: workers misunderstanding the task, missing context from the sentences, frames with overlapping or abstract definitions. The results show a clear link between inter-annotator disagreement and ambiguity, either in the sentence, frame, or the task itself. We argue that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating brittle, incomplete datasets, and therefore arbitrary targets for machine learning.  We further argued that ranking examples by a score is informative, and that the crowd offers alternate interpretations that are often sensible.


\section{Limitations \& Future Directions}

The research presented in this thesis has several possible directions for future work. In addition to the limitations specific to the material in each chapter, we identify three overarching issues to be explored in the future work: (1) expanding the experimental work on capturing ground truth ambiguity beyond relation extraction and frame disambiguation, (2) optimizing for the cost of data collection, and (3) building natural language processing models that learn to recognize ambiguity.


\subsection{Disagreement Beyond Relations and Frames}

%``Completeness proofs are notoriously difficult and are best avoided by any student who aims to finish his PhD on time.'' Judea Pearl in The Book of Why

To paraphrase Judea Pearl~\cite{pearl2018book}, proving completeness of a theory is notoriously difficult, and should be avoided if one wants to finish a PhD on time. This work does not claim completeness -- while we were able to successfully study the impact of disagreement on relation extraction and frame disambiguation, there are \textit{many more tasks and domains in natural language processing} that could be added to this analysis. Already, the CrowdTruth methodology for disagreement-preserving crowdsourcing has been applied to a variety of other tasks outside the scope of this thesis, such as named entity recognition~\cite{inel2017harnessing}, topical relevance of paragraphs~\cite{inel2018studying}, and textual description of videos~\cite{inel2018study}. Additionally, Section~\ref{sec:rel-work-ambig} discussed other ambiguity-prone tasks where our methodology for capturing and interpreting disagreement could be explored: anaphora resolution \cite{poesio2005reliability}, ontology alignment and evaluation~\cite{cheatham2014conference}, part-of-speech tagging~\cite{plank-hovy-sogaard:2014:P14-2}, and establishing grammatical correctness of text~\cite{lau2014measuring}.

Another interesting future direction would be to explore the \textit{compositionality of ambiguity}, as it applies to more complex natural language processing tasks, such as text summarization, machine translation, and question answering. We expect that ambiguity at the low-level of the text (e.g. ambiguous relations) will propagate and influence the ambiguity of the entire text. However, the compositional nature of language could potentially complicate the way this propagation occurs -- for instance, it is conceivable to have a text where every entity and relation is unambiguous, but when considering the whole text, ambiguity is present. This is frequently the case in legal texts~\cite{edelman1992legal}, which employ well-defined concepts but are usually open to multiple interpretations. A disagreement-based analysis of such texts could be used to identify the exact step in the language composition where ambiguity appears.

Finally, while the CrowdTruth method of aggregating crowdsourcing results has shown promising results, it should be \textit{compared with more baselines that go beyond majority vote}. In Section~\ref{sec:rel-work-agg}, we have presented several alternative crowdsourcing aggregation metrics~\cite{welinder2010multidimensional,werling2015job,NIPS2009_3644,Bozzon:2013,Kittur2008,Ipeirotis:2010}, out of which the most promising appear to be the Bayesian methods~\cite{paun2018comparing} that model worker reliability in combination with task difficulty. Future work should explore how the CrowdTruth approach compares to these methods in terms of quality of the ground truth they produce. Furthermore, it would be important to investigate whether Bayesian methods are able to identify ambiguity in the input data and annotations like CrowdTruth is doing.


\subsection{The Cost of Disagreement}

While in this thesis we have discussed how crowdsourcing is cheaper than domain experts, an analysis on how to optimize the cost of acquiring crowd annotations is still needed. To collect the different perspectives of the crowd, the CrowdTruth methodology uses a comparatively high number of annotators per task -- each task in this thesis used at least 10 workers per unit. Traditional crowdsourcing approaches tend to use less annotators, but this is not usually because of intentionally avoiding multiple perspectives, rather that the cost of employing many annotators is prohibitive.

In Chapter~\ref{chap:maj-vote}, we discussed the value in using a high number of workers per task, and also how the nature of the task (i.e. being more or less open and subjective) influences the optimal number of workers. Building on these results, an important future direction is to build a methodology for finding the optimal crowd payment and number of workers for a task, while also collecting the full spectrum of crowd opinions that can be expressed. A possible solution could be to implement an incremental method to collect annotations -- start with a smaller number of annotators for each input unit, then collect more judgments only if the smaller set of workers disagree.

Future work should compare CrowdTruth with other methods that optimize for cost of collection, like the general-purpose one proposed by \citet{Mizusawa:2018:EPP:3269206.3269292}. More specifically for the task of relation extraction, \citet{liu2016effective} proposed the Gated Crowd method to identify and train the highest skilled workers such that using only one worker per sentence is enough to bring significant improvement for the training of a relation extraction classifier. A comparison between Gated Crowd and CrowdTruth for relation extraction could be used as a starting point for a combined methodology, one that is able to separate between examples that need relabeling from a single highly skilled worker, and examples which are ambiguous and thus need multiple perspectives.


\subsection{Learning Ambiguity}

it's difficult to learn from ambiguity because it's mostly outliers

automatically detecting ambiguity~\cite{lebanoff2018automatic}
