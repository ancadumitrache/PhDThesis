% !TEX root = ../thesis.tex
%*******************************************************
% Summary
%*******************************************************
\phantomsection
\manualmark
\markboth{\spacedlowsmallcaps{Summary}}{\spacedlowsmallcaps{Summary}}
\addcontentsline{toc}{chapter}{\tocEntry{Summary}}

\chapter*{Summary}

Chapter~\ref{chap:med-rel-ex} explores this question for the task of medical relation extraction. In the medical domain it is typically assumed that expert annotators are required to get the best quality ground truth. This work shows that, by capturing the inter-annotator disagreement with the CrowdTruth method, medical relation classifiers trained on crowd annotations perform the same as those trained on expert annotations. Furthermore, classifiers trained on crowd annotations perform better than those trained with automatically-labeled data. Using the crowd also reduces the cost (monetary and in time required to find annotators) for collecting the data.

Chapter~\ref{chap:maj-vote} compares the performance of CrowdTruth metrics and majority vote, a consensus - enforcing metric, over a diverse set of crowdsourcing tasks. We show that applying the CrowdTruth methodology we collect richer data that allows us to reason about ambiguity of content. Furthermore, an increased number of crowd workers leads to growth and stabilization in the quality of annotations, going against the usual practice of employing a small number of annotators.

In Chapter~\ref{chap:od-rel-ex} we discuss how CrowdTruth data can be used to better models for relation classification for sentences. We build on work from Chapter~\ref{chap:med-rel-ex}, where we have shown that training models on on crowd annotations gives better results than training with data automatically-labeled with distant supervision~\cite{mintz2009distant}. However, crowd data is expensive to collect. Chapter~\ref{chap:od-rel-ex} describes how to correct a large corpus of training data for relation classification, using a relatively small crowdsourced corpus, using two different methods: (1) by manually propagating the false positive and cross-relation signals identified with the help of the crowd, and (2) by adapting the semantic label propagation method~\cite{sterckx2016knowledge} to work with CrowdTruth data.

In Chapter~\ref{chap:frames}, we explore this question as applied to the task of disambiguating semantic frames (i.e. high-level concepts that represent the meanings of words). Similarly to Chapter~\ref{chap:med-rel-ex}, we show that the crowd achieves comparative quality with domain experts. A qualitative evaluation of cases when crowd and expert disagree shows that inter-annotator disagreement is an indicator of ambiguity in both frames and sentences. We demonstrate that the cases in which the crowd workers could not agree exhibit ambiguity, either in the sentence, frame, or the task itself, arguing that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating arbitrary targets for machine learning.
